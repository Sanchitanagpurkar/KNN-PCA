{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment - KNN & PCA\n",
        "\n",
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "  - K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm that makes predictions based on the similarity (distance) between data points.\n",
        "\n",
        "      - Classification: KNN assigns a class to a new data point by looking at the majority class among its k nearest neighbors.\n",
        "\n",
        "      - Regression: KNN predicts a continuous value by taking the average (or weighted average) of the target values of its k nearest neighbors.\n",
        "\n",
        "  The performance depends heavily on:\n",
        "\n",
        "      - Choice of k (small k → overfitting, large k → underfitting)\n",
        "\n",
        "      - Choice of distance metric (Euclidean, Manhattan, etc.)\n",
        "\n",
        "      - Feature scaling, since KNN is distance-based.\n",
        "\n",
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "  - The Curse of Dimensionality refers to the fact that as the number of features (dimensions) increases:\n",
        "\n",
        "      - Data points become sparser.\n",
        "\n",
        "      - Distances between points become less meaningful (all points seem equidistant).\n",
        "\n",
        "      - Computational cost increases exponentially.\n",
        "\n",
        "  - In KNN, this causes distance metrics to lose discriminative power, making it harder to identify true nearest neighbors, thus degrading model accuracy.\n",
        "\n",
        "3.  What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "  - PCA is a dimensionality reduction technique that transforms original features into a new set of uncorrelated features (principal components) that maximize variance.\n",
        "\n",
        "      - PCA is feature extraction (creates new features).\n",
        "\n",
        "      - Feature selection simply chooses a subset of existing features without transforming them.\n",
        "\n",
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "  - Eigenvectors: Directions of maximum variance in the data (principal components).\n",
        "\n",
        "  - Eigenvalues: The amount of variance explained by each eigenvector.\n",
        "\n",
        "      They are important because:\n",
        "\n",
        "  - The largest eigenvalues indicate the most informative directions.\n",
        "\n",
        "  - The ratio of eigenvalues helps decide how many components to retain.\n",
        "\n",
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "  - PCA reduces dimensionality, removing noise and redundancy.\n",
        "\n",
        "  - This makes distance computations in KNN more effective, avoiding the curse of dimensionality.\n",
        "\n",
        "  - Together, PCA + KNN provide faster, more accurate, and robust models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JJ2wE1gKoirZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdCG9F4mnr2y",
        "outputId": "8cc415c7-c1e6-4b1a-9b16-0190ac17ba33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ],
      "source": [
        "# 6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(StandardScaler().fit_transform(X))\n",
        "\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrOJz1RkpFXa",
        "outputId": "46e37d3a-f71c-4863-eadb-b0d68cf2c9e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio: [0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(StandardScaler().fit_transform(X))\n",
        "\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn.predict(X_test_pca)\n",
        "print(\"Accuracy with PCA (2 components):\", accuracy_score(y_test, y_pred_pca))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dmp4dzSpFKx",
        "outputId": "42587997-ef25-4c2a-b76e-fea5bb178dae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "# Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_eu = knn_euclidean.predict(X_test_scaled)\n",
        "print(\"Accuracy with Euclidean:\", accuracy_score(y_test, y_pred_eu))\n",
        "\n",
        "# Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "print(\"Accuracy with Manhattan:\", accuracy_score(y_test, y_pred_manhattan))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCLwVQtLpE9A",
        "outputId": "b32df7c0-c7a7-426e-8662-3be9ec99b7a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean: 0.9629629629629629\n",
            "Accuracy with Manhattan: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "\n",
        "      Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "  \n",
        "  Explain how you would:\n",
        "\n",
        "      ● Use PCA to reduce dimensionality\n",
        "\n",
        "      ● Decide how many components to keep\n",
        "    \n",
        "      ● Use KNN for classification post-dimensionality reduction\n",
        "  \n",
        "      ● Evaluate the model\n",
        "\n",
        "      ● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "      (Include your Python code and output in the code box below.)\n",
        "  - Explanation for Stakeholders:\n",
        "\n",
        "      - PCA reduces thousands of gene features into a smaller set of informative components, removing noise and redundancy.\n",
        "\n",
        "      - Choosing components that explain 95% variance ensures we retain meaningful biological signals.\n",
        "\n",
        "      - KNN is a simple, interpretable model that works well after PCA since distance metrics become meaningful in reduced dimensions.\n",
        "\n",
        "      - This pipeline prevents overfitting, improves computational efficiency, and is a robust choice for biomedical data with small samples but many features."
      ],
      "metadata": {
        "id": "NKxh0UqvpTbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA + KNN on High-Dimensional Biomedical Data (Example with Cancer Dataset)\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (simulating gene expression dataset)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for PCA & KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# PCA: keep 95% variance (decides components automatically)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Number of components retained:\", pca.n_components_)\n",
        "\n",
        "# Train KNN on PCA-transformed data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Accuracy with PCA + KNN:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ElhgPgmpqfv",
        "outputId": "09fa4737-3e53-4a36-d86e-142b7d68e101"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components retained: 10\n",
            "Accuracy with PCA + KNN: 0.9649122807017544\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95        63\n",
            "           1       0.96      0.98      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.97      0.96      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ]
    }
  ]
}